{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import time\nimport copy\nimport numpy as np\nimport pandas as pd\nfrom datetime import datetime\nfrom statistics import mean\n\nimport torch\nimport torch.nn as nn\nfrom torch import optim\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.utils.data import SubsetRandomSampler\n\nimport torchvision\nimport torchvision.models as models\nimport torchvision.transforms as transforms\n\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom PIL import Image\nfrom itertools import combinations\n\nimport random\nfrom tqdm import tqdm # Progress Bar\n\nimport warnings\n# Ignore all warnings\nwarnings.filterwarnings(\"ignore\")\nfrom statistics import mean\n\nimport cv2\n\nimport itertools\nimport random\nimport os\n\nfrom sklearn.metrics import classification_report, confusion_matrix","metadata":{"execution":{"iopub.status.busy":"2023-07-22T11:21:59.006144Z","iopub.execute_input":"2023-07-22T11:21:59.006565Z","iopub.status.idle":"2023-07-22T11:22:03.813941Z","shell.execute_reply.started":"2023-07-22T11:21:59.006531Z","shell.execute_reply":"2023-07-22T11:22:03.812847Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Device","metadata":{}},{"cell_type":"code","source":"# Move the processed image to a GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ndevice","metadata":{"execution":{"iopub.status.busy":"2023-07-22T11:22:03.817051Z","iopub.execute_input":"2023-07-22T11:22:03.819262Z","iopub.status.idle":"2023-07-22T11:22:03.897192Z","shell.execute_reply.started":"2023-07-22T11:22:03.819207Z","shell.execute_reply":"2023-07-22T11:22:03.895766Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Creating Dataset\n\nTwo different dataset are created in order for training:\n- First Triplet Dataset is created for training the Resnet18 model in Triplet Loss.\n- Second Duplet Dataset is created for logisitic regression for classification.","metadata":{}},{"cell_type":"code","source":"def random_images(dataset_folder):\n    random_images = []\n    for person_folder in os.listdir(dataset_folder):\n        person_folder_path = os.path.join(dataset_folder, person_folder)\n        filenames = os.listdir(person_folder_path)\n        if len(filenames) > 7:\n            filenames = random.sample(filenames, 7)  # Select 7 random filenames\n        for filename in filenames:\n            random_images.append(os.path.join(person_folder_path, filename))\n    return random_images","metadata":{"execution":{"iopub.status.busy":"2023-07-22T11:22:03.8996Z","iopub.execute_input":"2023-07-22T11:22:03.900369Z","iopub.status.idle":"2023-07-22T11:22:03.914319Z","shell.execute_reply.started":"2023-07-22T11:22:03.900332Z","shell.execute_reply":"2023-07-22T11:22:03.913193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to create triplets for triplet training.\ndef triplet_dataset_preparation(dataset_folder):\n    image_paths = random_images(dataset_folder)\n    # Create an empty DataFrame\n    df = pd.DataFrame(columns=['Anchor_Path', 'Positive_Path', 'Negative_Path'])\n\n    for person_folder in os.listdir(dataset_folder):\n        person_folder_path = os.path.join(dataset_folder, person_folder)\n        genuine_images = []\n        forged_images = []\n        for filename in os.listdir(person_folder_path):\n            if 'original' in filename or '-G-' in filename:\n                genuine_images.append(os.path.join(person_folder_path,filename))\n            if 'forgeries' in filename or '-F-' in filename or 'forge' in filename:\n                forged_images.append(os.path.join(person_folder_path,filename))\n                \n        additional_images = random.sample(image_paths, 10)\n        forged_images.extend(additional_images)\n        \n        num_combinations = min(len(genuine_images) * (len(genuine_images) - 1) // 2, len(genuine_images) * len(forged_images))\n        genuine_combinations = random.sample(list(itertools.combinations(genuine_images, 2)), num_combinations)\n        forged_combinations = random.sample(list(itertools.product(genuine_images, forged_images)), num_combinations)\n\n        # Create a DataFrame with the balanced triplets\n        data = []\n        for (image_1, image_2), (genuine_image, forged_image) in zip(genuine_combinations, forged_combinations):\n            anchor_path = os.path.join(image_1)\n            positive_path = os.path.join(image_2)\n            negative_path = os.path.join(forged_image)\n            data.append([anchor_path, positive_path, negative_path])\n\n        df = df.append(pd.DataFrame(data, columns=['Anchor_Path', 'Positive_Path', 'Negative_Path']), ignore_index=True)\n    return df\n\n# function to create duplets for logisitic regression training\ndef duplet_dataset_preparation(dataset_folder):\n    image_paths = random_images(dataset_folder)\n    # Create an empty DataFrame\n    df = pd.DataFrame(columns=['Image1', 'Image2', 'Label'])\n\n    for person_folder in os.listdir(dataset_folder):\n\n        person_folder_path = os.path.join(dataset_folder, person_folder)\n\n        genuine_images = []\n        forged_images = []\n        for filename in os.listdir(person_folder_path):\n            if 'original' in filename or '-G-' in filename:\n                genuine_images.append(os.path.join(person_folder_path,filename))\n            if 'forgeries' in filename or '-F-' in filename or 'forge' in filename:\n                forged_images.append(os.path.join(person_folder_path, filename))\n\n        additional_images = random.sample(image_paths, 10)\n        forged_images.extend(additional_images)\n        num_genuine_images = len(genuine_images)\n        num_forged_images = len(forged_images)\n        num_combinations = min(num_genuine_images * (num_genuine_images - 1) // 2, num_genuine_images * num_forged_images)\n        genuine_combinations = random.sample(list(itertools.combinations(genuine_images, 2)), num_combinations)\n        forged_combinations = random.sample(list(itertools.product(genuine_images, forged_images)), num_combinations)\n\n        # Create a DataFrame with the balanced combinations\n        data = []\n        for (image_1, image_2), (genuine_image, forged_image) in zip(genuine_combinations, forged_combinations):\n            anchor_path = os.path.join(image_1)\n            positive_path = os.path.join(image_2)\n            label = 0\n            data.append([anchor_path, positive_path, label])\n\n            anchor_path = os.path.join(genuine_image)\n            positive_path = os.path.join(forged_image)\n            label = 1\n            data.append([anchor_path, positive_path, label])\n\n        df = df.append(pd.DataFrame(data, columns=['Image1', 'Image2', 'Label']), ignore_index=True)\n    return df","metadata":{"execution":{"iopub.status.busy":"2023-07-22T11:22:03.918945Z","iopub.execute_input":"2023-07-22T11:22:03.919213Z","iopub.status.idle":"2023-07-22T11:22:03.939817Z","shell.execute_reply.started":"2023-07-22T11:22:03.91919Z","shell.execute_reply":"2023-07-22T11:22:03.938749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cedar_dataset = '/kaggle/input/signature-forgery-dataset/Train'\ncedar_df = triplet_dataset_preparation(cedar_dataset)\n\n# cedar_df = cedar_df.sample(15000, random_state=42)\nprint(cedar_df.shape)","metadata":{"execution":{"iopub.status.busy":"2023-07-22T11:22:03.941065Z","iopub.execute_input":"2023-07-22T11:22:03.941379Z","iopub.status.idle":"2023-07-22T11:22:05.445786Z","shell.execute_reply.started":"2023-07-22T11:22:03.941348Z","shell.execute_reply":"2023-07-22T11:22:05.44488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cedar_df.sample(10)","metadata":{"execution":{"iopub.status.busy":"2023-07-22T11:22:05.4504Z","iopub.execute_input":"2023-07-22T11:22:05.452915Z","iopub.status.idle":"2023-07-22T11:22:05.477898Z","shell.execute_reply.started":"2023-07-22T11:22:05.45288Z","shell.execute_reply":"2023-07-22T11:22:05.476904Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Dataset Interface","metadata":{}},{"cell_type":"code","source":"class TripletDataset(Dataset):\n    \n    def __init__(self, training_df=None,transform=None):\n        self.training_df = training_df\n        self.training_df.columns = [\"Anchor_Path\", \"Positive_Path\", \"Negative_Path\"]  \n        self.transform = transform\n\n    def __getitem__(self, index):\n        # Getting the image paths\n        anchor_path = os.path.join(self.training_df.iat[int(index), 0])\n        positive_path = os.path.join(self.training_df.iat[int(index), 1])\n        negative_path = os.path.join(self.training_df.iat[int(index), 2])\n\n        # Loading the images\n        anchor_img = Image.open(anchor_path)\n        positive_img = Image.open(positive_path)\n        negative_img = Image.open(negative_path)\n        \n        # preprocess the image\n        anchor_img = preprocess_image(anchor_img)\n        positive_img = preprocess_image(positive_img)\n        negative_img = preprocess_image(negative_img)\n        \n        # Apply image transformations\n        if self.transform is not None:\n            anchor_img = self.transform(anchor_img)\n            positive_img = self.transform(positive_img)\n            negative_img = self.transform(negative_img)\n\n        return anchor_img, positive_img, negative_img\n\n    def __len__(self):\n        return len(self.training_df)\n\n\nclass DupletDataset(Dataset):\n    \n    def __init__(self,dataframe=None,transform=None):\n        # used to prepare the labels and images path\n        self.training_df=dataframe\n        self.training_df.columns =[\"image1\",\"image2\",\"label\"]   \n        self.transform = transform\n\n    def __getitem__(self,index):\n        \n        # getting the image path\n        image1_path=os.path.join(self.training_df.iat[int(index),0])\n        image2_path=os.path.join(self.training_df.iat[int(index),1])\n        \n        # Loading the image\n        img0 = Image.open(image1_path)\n        img1 = Image.open(image2_path)\n        \n        img0 = preprocess_image(img0)\n        img1 = preprocess_image(img1)\n        \n        # Apply image transformations\n        if self.transform is not None:\n            img0 = self.transform(img0)\n            img1 = self.transform(img1)\n        label = torch.from_numpy(np.array([int(self.training_df.iat[int(index), 2])], dtype=np.float32))\n        \n        return img0, img1 , label\n    \n    def __len__(self):\n        return len(self.training_df)\n    ","metadata":{"execution":{"iopub.status.busy":"2023-07-22T11:22:05.482421Z","iopub.execute_input":"2023-07-22T11:22:05.48491Z","iopub.status.idle":"2023-07-22T11:22:05.505909Z","shell.execute_reply.started":"2023-07-22T11:22:05.484872Z","shell.execute_reply":"2023-07-22T11:22:05.504881Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Preprocessing of Image:\n- ***Convert Image to grayscale***:\n    The image is first converted to grayscale to simplify the data and reduce the computational load. Grayscale images contain only shades of gray and no color information.\n\n- ***MedianBlur for enhancing edge details***:\n    Median blur is applied to the grayscale image to reduce noise and enhance edge details. It replaces each pixel's value with the median value of its neighboring pixels, effectively reducing the impact of outliers or random noise.\n\n- ***Erosion followed by dilation***:\n    Erosion and dilation are morphological operations used to remove small noise and fill gaps in the image. Erosion erodes away the boundaries of the foreground object, and dilation enlarges the boundaries of the foreground object.\n\n- ***Binarization***:\n    After the erosion and dilation, the image is binarized to create a binary image with only two pixel values (usually 0 and 255). Binarization simplifies the image and helps in separating the foreground from the background.\n\n- ***Cropping of signature***:\n    Finally, the preprocessed image is cropped to extract the signature or the region of interest from the rest of the image. Cropping focuses on the essential part of the image, which in this case is the signature.","metadata":{}},{"cell_type":"code","source":"\ndef preprocess_image(image_pth):\n    \n    # Convert image to grayscale\n    gray = image_pth.convert(\"L\")\n\n    # Convert grayscale image to numpy array\n    img = np.array(gray)\n    # Apply median blur\n    blur = cv2.medianBlur(img,3)\n\n    # Define kernel for morphology operations\n    kernel = cv2.getStructuringElement(cv2.MORPH_CROSS, (3, 3))\n\n    # Perform erosion\n    erode = cv2.erode(blur, kernel, iterations=2)\n\n    # Perform dilation\n    dilate = cv2.dilate(erode, kernel, iterations=1)\n\n    # Apply thresholding\n    _, binary = cv2.threshold(dilate, 128, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n    # Find the bounding box coordinates of the non-white pixels\n    coords = cv2.findNonZero(binary)\n    x, y, w, h = cv2.boundingRect(coords)\n\n    # Add extra white space to the bounding box coordinates\n    padding = 20  # Adjust the padding size as needed\n    x -= padding\n    y -= padding\n    w += 2 * padding\n    h += 2 * padding\n\n    # Make sure the coordinates are within the image boundaries\n    x = max(0, x)\n    y = max(0, y)\n    w = min(w, img.shape[1] - x)\n    h = min(h, img.shape[0] - y)\n\n    # Crop the image using the modified bounding box coordinates\n    cropped_image = binary[y:y+h, x:x+w]\n\n    # Add extra white space around the cropped image\n    extra_space = np.zeros((cropped_image.shape[0] + 2 * padding, cropped_image.shape[1] + 2 * padding), dtype=np.uint8) * 255\n    extra_space[padding:-padding, padding:-padding] = cropped_image\n    \n    # Convert the numpy array back to PIL image\n    resized_image = Image.fromarray(extra_space)\n\n    return resized_image","metadata":{"execution":{"iopub.status.busy":"2023-07-22T11:22:05.51098Z","iopub.execute_input":"2023-07-22T11:22:05.514035Z","iopub.status.idle":"2023-07-22T11:22:05.529004Z","shell.execute_reply.started":"2023-07-22T11:22:05.513997Z","shell.execute_reply":"2023-07-22T11:22:05.527864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef plot_images(img):\n    fig, axes = plt.subplots(1,2, figsize=(10,10))\n    axes[0].imshow(img)\n    axes[0].axis('off')\n    axes[0].set_title(\"before_preprocessing\")\n\n    after_preprocessing = preprocess_image(img)\n    print(after_preprocessing.size)\n    axes[1].imshow(after_preprocessing)\n    axes[1].axis('off')\n    axes[1].set_title(\"after_preprocessing\")\n","metadata":{"execution":{"iopub.status.busy":"2023-07-22T11:22:05.53333Z","iopub.execute_input":"2023-07-22T11:22:05.535619Z","iopub.status.idle":"2023-07-22T11:22:05.547105Z","shell.execute_reply.started":"2023-07-22T11:22:05.535585Z","shell.execute_reply":"2023-07-22T11:22:05.546353Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img = Image.open(\"/kaggle/input/handwritten-signature-datasets/CEDAR/CEDAR/5/forgeries_5_20.png\")\nplot_images(img)","metadata":{"execution":{"iopub.status.busy":"2023-07-22T11:22:05.553804Z","iopub.execute_input":"2023-07-22T11:22:05.55617Z","iopub.status.idle":"2023-07-22T11:22:06.103404Z","shell.execute_reply.started":"2023-07-22T11:22:05.556137Z","shell.execute_reply":"2023-07-22T11:22:06.102367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img = Image.open(\"/kaggle/input/signature-forgery-dataset/Test/6/original_06_02.jpg\")\nplot_images(img)","metadata":{"execution":{"iopub.status.busy":"2023-07-22T11:22:06.10459Z","iopub.execute_input":"2023-07-22T11:22:06.104994Z","iopub.status.idle":"2023-07-22T11:22:06.444685Z","shell.execute_reply.started":"2023-07-22T11:22:06.104957Z","shell.execute_reply":"2023-07-22T11:22:06.443728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transformation = transforms.Compose([\n    transforms.Resize((200,300)),\n    transforms.RandomRotation((-5,10)),\n    transforms.ToTensor(),\n])","metadata":{"execution":{"iopub.status.busy":"2023-07-22T11:22:06.446495Z","iopub.execute_input":"2023-07-22T11:22:06.447241Z","iopub.status.idle":"2023-07-22T11:22:06.452949Z","shell.execute_reply.started":"2023-07-22T11:22:06.447204Z","shell.execute_reply":"2023-07-22T11:22:06.451771Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def show_triplets_from_dataloader(dataloader, num_triplets=3):\n    for batch_idx, (anchor_imgs, positive_imgs, negative_imgs) in enumerate(dataloader):\n        for i in range(len(anchor_imgs)):\n            if i >= num_triplets:\n                break\n\n            # Display the anchor, positive, and negative images\n            fig, axes = plt.subplots(1, 3, figsize=(8, 4))\n            axes[0].imshow(anchor_imgs[i].permute(1, 2, 0))  # Permute dimensions for image display (assuming tensor input)\n            axes[0].set_title(\"Anchor\")\n            axes[1].imshow(positive_imgs[i].permute(1, 2, 0))\n            axes[1].set_title(\"Positive\")\n            axes[2].imshow(negative_imgs[i].permute(1, 2, 0))\n            axes[2].set_title(\"Negative\")\n            plt.tight_layout()\n            plt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-07-22T11:22:06.454792Z","iopub.execute_input":"2023-07-22T11:22:06.455464Z","iopub.status.idle":"2023-07-22T11:22:06.465691Z","shell.execute_reply.started":"2023-07-22T11:22:06.45543Z","shell.execute_reply":"2023-07-22T11:22:06.464826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import DataLoader, SubsetRandomSampler\nimport torchvision.transforms as transforms\n\ndataset =  TripletDataset(cedar_df, transform = transformation)\n\nbatch_size = 32\nloader = torch.utils.data.DataLoader(dataset, batch_size= batch_size)","metadata":{"execution":{"iopub.status.busy":"2023-07-22T11:22:06.467208Z","iopub.execute_input":"2023-07-22T11:22:06.467993Z","iopub.status.idle":"2023-07-22T11:22:06.477348Z","shell.execute_reply.started":"2023-07-22T11:22:06.467958Z","shell.execute_reply":"2023-07-22T11:22:06.476673Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import DataLoader, SubsetRandomSampler\n# Determine the number of samples to show\nnum_samples_to_show = 5\n\n# Get random indices to select samples from the DataLoader\nnum_samples = len(loader.dataset)\nrandom_indices = np.random.choice(num_samples, num_samples_to_show, replace=False)\n\n# Create a SubsetRandomSampler using the random indices\nsampler = SubsetRandomSampler(random_indices)\n\n# Create a new DataLoader using the SubsetRandomSampler\nrandom_loader = DataLoader(dataset, batch_size=1, sampler=sampler)\n\n# Show the random samples\nshow_triplets_from_dataloader(random_loader, num_samples_to_show)","metadata":{"execution":{"iopub.status.busy":"2023-07-22T11:22:06.478851Z","iopub.execute_input":"2023-07-22T11:22:06.479479Z","iopub.status.idle":"2023-07-22T11:22:09.989015Z","shell.execute_reply.started":"2023-07-22T11:22:06.479447Z","shell.execute_reply":"2023-07-22T11:22:09.987999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(loader)","metadata":{"execution":{"iopub.status.busy":"2023-07-22T11:22:09.99032Z","iopub.execute_input":"2023-07-22T11:22:09.991942Z","iopub.status.idle":"2023-07-22T11:22:09.998542Z","shell.execute_reply.started":"2023-07-22T11:22:09.991906Z","shell.execute_reply":"2023-07-22T11:22:09.997574Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## ResNet18 with Spatial Attention","metadata":{}},{"cell_type":"code","source":"class SpatialAttention(nn.Module):\n    def __init__(self, in_channels):\n        super(SpatialAttention, self).__init__()\n        self.conv = nn.Conv2d(in_channels, 1, kernel_size=1)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        # Compute attention weights\n        attention_scores = self.conv(x)\n        attention_weights = self.sigmoid(attention_scores)\n\n        # Apply attention to the input feature map\n        attended_features = x * attention_weights\n\n        return attended_features\n\n\nclass SiameseResNet(nn.Module):\n    def __init__(self, model_name='resnet18', pretrained=False):\n        super(SiameseResNet, self).__init__()\n        self.baseModel = models.resnet18(pretrained=pretrained)\n\n        # Experiment with different spatial sizes based on the image resolution and signature complexity\n        self.attention1 = SpatialAttention(in_channels=64)  # Spatial attention for layer 1\n        self.attention2 = SpatialAttention(in_channels=128)  # Spatial attention for layer 2\n\n        self.baseModel.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n        self.baseModel.fc = nn.Identity()\n\n    def forward(self, x):\n        out = self.baseModel.conv1(x)\n        out = self.baseModel.bn1(out)\n        out = self.baseModel.relu(out)\n        out = self.baseModel.maxpool(out)\n\n        out = self.attention1(self.baseModel.layer1(out))  # Applying spatial attention to layer 1\n        out = self.attention2(self.baseModel.layer2(out))  # Applying spatial attention to layer 2\n        out = self.baseModel.layer3(out)  # No attention for layer 3\n        out = self.baseModel.layer4(out)  # No attention for layer 4\n\n        out = self.baseModel.avgpool(out)\n        out = torch.flatten(out, 1)\n        return out","metadata":{"execution":{"iopub.status.busy":"2023-07-22T11:22:10.000227Z","iopub.execute_input":"2023-07-22T11:22:10.001081Z","iopub.status.idle":"2023-07-22T11:22:10.014198Z","shell.execute_reply.started":"2023-07-22T11:22:10.001007Z","shell.execute_reply":"2023-07-22T11:22:10.013125Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Triplet Loss","metadata":{}},{"cell_type":"code","source":"class TripletLoss(nn.Module):\n    def __init__(self, margin=1.0):\n        super(TripletLoss, self).__init__()\n        self.margin = margin\n\n    def forward(self, anchor, positive, negative):\n        distance_anchor_positive = F.pairwise_distance(anchor, positive, p=2)\n        distance_anchor_negative = F.pairwise_distance(anchor, negative, p=2)\n        loss = torch.clamp(distance_anchor_positive - distance_anchor_negative + self.margin, min=0.0)\n        return loss.mean()\n","metadata":{"execution":{"iopub.status.busy":"2023-07-22T11:24:34.510567Z","iopub.execute_input":"2023-07-22T11:24:34.511304Z","iopub.status.idle":"2023-07-22T11:24:34.519896Z","shell.execute_reply.started":"2023-07-22T11:24:34.51126Z","shell.execute_reply":"2023-07-22T11:24:34.518908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_model(model, loader, n_epochs, optimizer, loss_fn):\n    history = {'train_loss': []}\n    print(\"------------------------Training--------------------------\")\n    for epoch in range(1, n_epochs + 1):\n        t0 = datetime.now()\n        print(f\"Beginning Epoch {epoch}/{n_epochs}...\")\n        train_loss = []\n        model.train()\n        for i, data in tqdm(enumerate(loader, 0)):\n            anchor, positive, negative = data\n            anchor = anchor.to(device=device)\n            positive = positive.to(device=device)\n            negative = negative.to(device=device)\n            \n            optimizer.zero_grad()\n            anchor_embeddings = model(anchor)  \n            positive_embeddings = model(positive)  \n            negative_embeddings = model(negative) \n\n            loss = loss_fn(anchor_embeddings, positive_embeddings, negative_embeddings)  # Changed `triplet_loss` to `loss_fn`\n            \n            loss.backward()\n            optimizer.step()\n            train_loss.append(loss.item())  # Added the loss value to `train_loss` list\n\n        dt = datetime.now() - t0\n        print('\\nEpoch: {}\\tTrain Loss: {}\\tDuration: {}'.format(epoch, np.mean(train_loss), dt))\n\n        # Tracking accuracy and loss in each epoch for plot\n        history['train_loss'].append(np.mean(train_loss))\n    \n    return history\n","metadata":{"execution":{"iopub.status.busy":"2023-07-22T11:24:55.976576Z","iopub.execute_input":"2023-07-22T11:24:55.977268Z","iopub.status.idle":"2023-07-22T11:24:55.988093Z","shell.execute_reply.started":"2023-07-22T11:24:55.977231Z","shell.execute_reply":"2023-07-22T11:24:55.98572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nmargin = 0.1\n\n# Create an instance of SiameseResnet with the ResNet model and embedding size\nsiamese_model = SiameseResNet()\nsiamese_model = nn.DataParallel(siamese_model).to(device)\n\ntriplet_loss = TripletLoss(margin).to(device)\noptimizer = torch.optim.Adam(siamese_model.parameters(), lr=0.001)\n","metadata":{"execution":{"iopub.status.busy":"2023-07-22T11:26:04.380148Z","iopub.execute_input":"2023-07-22T11:26:04.380521Z","iopub.status.idle":"2023-07-22T11:26:04.679475Z","shell.execute_reply.started":"2023-07-22T11:26:04.380493Z","shell.execute_reply":"2023-07-22T11:26:04.678446Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install torchsummary","metadata":{"execution":{"iopub.status.busy":"2023-07-22T11:22:13.457014Z","iopub.execute_input":"2023-07-22T11:22:13.457361Z","iopub.status.idle":"2023-07-22T11:22:26.253235Z","shell.execute_reply.started":"2023-07-22T11:22:13.457328Z","shell.execute_reply":"2023-07-22T11:22:26.251827Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torchsummary import summary\n\nmodel = SiameseResNet()\nmodel = nn.DataParallel(model).to(device)\nsummary(model, (1,200,300))","metadata":{"execution":{"iopub.status.busy":"2023-07-22T11:22:26.25695Z","iopub.execute_input":"2023-07-22T11:22:26.257267Z","iopub.status.idle":"2023-07-22T11:22:35.506598Z","shell.execute_reply.started":"2023-07-22T11:22:26.257239Z","shell.execute_reply":"2023-07-22T11:22:35.505623Z"},"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(siamese_model)","metadata":{"execution":{"iopub.status.busy":"2023-07-22T11:22:35.508256Z","iopub.execute_input":"2023-07-22T11:22:35.508953Z","iopub.status.idle":"2023-07-22T11:22:35.517739Z","shell.execute_reply.started":"2023-07-22T11:22:35.508917Z","shell.execute_reply":"2023-07-22T11:22:35.516834Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_epochs = 15\n  \nhistory = train_model(siamese_model, loader, num_epochs, optimizer, triplet_loss)","metadata":{"execution":{"iopub.status.busy":"2023-07-22T11:26:08.478164Z","iopub.execute_input":"2023-07-22T11:26:08.478522Z","iopub.status.idle":"2023-07-22T11:26:19.984331Z","shell.execute_reply.started":"2023-07-22T11:26:08.478491Z","shell.execute_reply":"2023-07-22T11:26:19.982627Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(range(1, 16), history['train_loss'], 'b', label='Triplet Loss')\nplt.title('Triplet Loss of Model', fontsize=16)\nplt.xlabel('Epochs', fontsize=14)\nplt.ylabel('Loss', fontsize=14)\nplt.legend()","metadata":{"execution":{"iopub.status.busy":"2023-07-22T11:22:37.04713Z","iopub.status.idle":"2023-07-22T11:22:37.04774Z","shell.execute_reply.started":"2023-07-22T11:22:37.047443Z","shell.execute_reply":"2023-07-22T11:22:37.047465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(siamese_model.state_dict(),'resnet_with_attention_triplet_saturday.pth') ","metadata":{"execution":{"iopub.status.busy":"2023-07-22T11:22:37.052349Z","iopub.status.idle":"2023-07-22T11:22:37.052732Z","shell.execute_reply.started":"2023-07-22T11:22:37.052525Z","shell.execute_reply":"2023-07-22T11:22:37.052542Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Logistic Regression","metadata":{}},{"cell_type":"code","source":"import torch.nn.functional as F\n\nclass LogisticSiameseRegression(nn.Module):\n    def __init__(self, model):\n        super(LogisticSiameseRegression, self).__init__()\n        \n        self.model = model\n        self.fc = nn.Sequential(\n            nn.Linear(512, 128),\n            nn.ReLU(),\n            nn.Linear(128,1)\n        )\n        self.sigmoid = nn.Sigmoid()\n        \n    def forward_once(self, x):\n        out = self.model(x)\n        out = F.normalize(out, p=2, dim=1)  # L2 normalization\n        return out\n    \n    def forward(self, x1, x2):\n        out1 = self.forward_once(x1)\n        out2 = self.forward_once(x2)\n        diff = out1 - out2\n        out = self.fc(diff)\n        out = self.sigmoid(out)\n        return out","metadata":{"execution":{"iopub.status.busy":"2023-07-22T11:22:37.054288Z","iopub.status.idle":"2023-07-22T11:22:37.054835Z","shell.execute_reply.started":"2023-07-22T11:22:37.054574Z","shell.execute_reply":"2023-07-22T11:22:37.0546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train(model, input1, input2, outputs, optimizer, loss_fn):\n    # Set the model to training mode\n    model.train()\n    # Zero the gradients\n    optimizer.zero_grad()\n    # Compute the model's predictions\n    predictions = model(input1, input2)\n    # Compute the loss\n    loss = loss_fn(predictions, outputs)\n    # Compute the gradients\n    loss.backward()\n    # Update the weights\n    optimizer.step()\n    return loss, predictions\n\n\ndef train_model(model, train_loader, test_loader, n_epochs, optimizer, loss_fn):\n    history = {'train_loss': [], 'train_acc': [], 'valid_loss': [], 'valid_acc': []}\n    print(\"------------------------Training--------------------------\")\n\n    for epoch in range(1, n_epochs + 1):\n        t0 = datetime.now()\n        print(f\"Beginning Epoch {epoch}/{n_epochs}...\")\n        train_loss = []\n        train_acc = []\n        model.train()\n        for i, data in tqdm(enumerate(train_loader, 0)):\n            inputs1, inputs2, targets = data\n            inputs1 = inputs1.to(device=device)\n            inputs2 = inputs2.to(device=device)\n            targets = targets.to(device=device)\n\n            loss, predictions = train(model, inputs1, inputs2, targets, optimizer, loss_fn)\n            train_loss.append(loss.item())\n            accuracy = (predictions.round() == targets).float().mean().item()\n            train_acc.append(accuracy)\n\n        valid_loss = []\n        valid_acc = []\n        model.eval()\n        with torch.no_grad():\n            for i, data in tqdm(enumerate(test_loader, 0)):\n                inputs1, inputs2, targets = data\n                inputs1 = inputs1.to(device=device)\n                inputs2 = inputs2.to(device=device)\n                targets = targets.to(device=device)\n\n                output = model(inputs1, inputs2)\n                loss = loss_fn(output, targets)\n                valid_loss.append(loss.item())\n                accuracy = (output.round() == targets).float().mean().item()\n                valid_acc.append(accuracy)\n\n        dt = datetime.now() - t0\n        print('\\nEpoch: {}\\t\\tTrain Loss: {:.4f}\\tTrain Accuracy: {:.4f}\\nDuration: {}\\tValid Loss: {:.4f}\\tValid Accuracy: {:.4f}\\n'.format(\n             epoch, np.mean(train_loss), np.mean(train_acc), dt, np.mean(valid_loss), np.mean(valid_acc)\n        ))\n\n        # Tracking accuracy and loss in each epoch for plot\n        history['train_loss'].append(np.mean(train_loss))\n        history['train_acc'].append(np.mean(train_acc))\n        history['valid_loss'].append(np.mean(valid_loss))\n        history['valid_acc'].append(np.mean(valid_acc))\n    \n    return history","metadata":{"execution":{"iopub.status.busy":"2023-07-22T11:22:37.056722Z","iopub.status.idle":"2023-07-22T11:22:37.057186Z","shell.execute_reply.started":"2023-07-22T11:22:37.056941Z","shell.execute_reply":"2023-07-22T11:22:37.056963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cedar_dataset = '/kaggle/input/signature-forgery-dataset/Train'\ncedar_duplet = duplet_dataset_preparation(cedar_dataset)\n\n# cedar_duplet = cedar_duplet.sample(10000, random_state=42)\ncedar_dataset = DupletDataset(cedar_duplet, transform=transformation)\nindices = list(range(len(cedar_dataset)))\nsplit = int(np.floor(0.80 * len(cedar_dataset)))  # train_size\nvalidation = int(np.floor(0.70 * split))   # validation\nnp.random.shuffle(indices)\n\ntrain_indices, validation_indices, test_indices = (\n    indices[:validation],\n    indices[validation:split],\n    indices[split:],\n)\ntrain_sampler = SubsetRandomSampler(train_indices)\nvalidation_sampler = SubsetRandomSampler(validation_indices)\ntest_sampler = SubsetRandomSampler(test_indices)\n\nbatch_size = 32\ntrain_loader = torch.utils.data.DataLoader(\n    cedar_dataset, batch_size=batch_size, sampler=train_sampler\n)\ntest_loader = torch.utils.data.DataLoader(\n    cedar_dataset, batch_size=batch_size, sampler=test_sampler\n)\nvalidation_loader = torch.utils.data.DataLoader(\n    cedar_dataset, batch_size=batch_size, sampler=validation_sampler\n)\n","metadata":{"execution":{"iopub.status.busy":"2023-07-22T11:22:37.059009Z","iopub.status.idle":"2023-07-22T11:22:37.059479Z","shell.execute_reply.started":"2023-07-22T11:22:37.059242Z","shell.execute_reply":"2023-07-22T11:22:37.059265Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_rms = LogisticSiameseRegression(siamese_model).to(device)\nloss_fn = nn.BCELoss().to(device)\noptimizer = torch.optim.Adam(model_rms.parameters(), lr=0.001)\n\nhistory = train_model(model_rms, train_loader, test_loader, 10, optimizer, loss_fn)","metadata":{"execution":{"iopub.status.busy":"2023-07-22T11:22:37.061584Z","iopub.status.idle":"2023-07-22T11:22:37.062686Z","shell.execute_reply.started":"2023-07-22T11:22:37.06244Z","shell.execute_reply":"2023-07-22T11:22:37.062463Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(model_rms.state_dict(),'model_final_attention_saturday.pth')","metadata":{"execution":{"iopub.status.busy":"2023-07-22T11:22:37.063917Z","iopub.status.idle":"2023-07-22T11:22:37.064718Z","shell.execute_reply.started":"2023-07-22T11:22:37.064462Z","shell.execute_reply":"2023-07-22T11:22:37.064484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_acc_loss(history, num_epochs):\n    \n    fig, axes = plt.subplots(1, 2, figsize=(15, 10))\n    \n    axes[0].plot(range(1, num_epochs+1), history['train_acc'], 'b', label='Training Accuracy')\n    axes[0].plot(range(1, num_epochs+1), history['valid_acc'], 'r', label='Validation Accuracy')\n    axes[0].set_title('Training and Validation Accuracy of Model', fontsize=16)\n    axes[0].set_xlabel('Epochs', fontsize=14)\n    axes[0].set_ylabel('Accuracy', fontsize=14)\n    axes[0].legend()\n    \n    axes[1].plot(range(1, num_epochs+1), history['train_loss'], 'b', label='Training Loss')\n    axes[1].plot(range(1, num_epochs+1), history['valid_loss'], 'r', label='Validation Loss')\n    axes[1].set_title('Training and Validation Loss of Model', fontsize=16)\n    axes[1].set_xlabel('Epochs', fontsize=14)\n    axes[1].set_ylabel('Loss', fontsize=14)\n    axes[1].legend()\n    \n    plt.show()\n\nplot_acc_loss(history, 10)\n","metadata":{"execution":{"iopub.status.busy":"2023-07-22T11:22:37.066267Z","iopub.status.idle":"2023-07-22T11:22:37.067067Z","shell.execute_reply.started":"2023-07-22T11:22:37.06682Z","shell.execute_reply":"2023-07-22T11:22:37.066842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Testing","metadata":{}},{"cell_type":"code","source":"def calculate_accuracy(data_loader, model):\n    model.eval()  # Set the model to evaluation mode\n    correct = 0\n    total = 0\n    \n    with torch.no_grad():\n        for data in data_loader:\n            inputs1, inputs2, targets = data\n            inputs1 = inputs1.to(device)\n            inputs2 = inputs2.to(device)\n            targets = targets.to(device)\n\n            outputs = model(inputs1, inputs2)\n            predictions = (outputs > 0.5).float()  # Convert probabilities to binary predictions\n            correct += (predictions == targets).sum().item()\n            total += targets.size(0)\n    \n    accuracy = correct / total * 100\n    return accuracy\n\ndef get_predictions(data_loader, model):\n    all_preds = []\n    all_labels = []\n    model.eval()\n    for i, data in tqdm(enumerate(data_loader, 0)):\n        inputs1, inputs2, targets = data\n        inputs1 = inputs1.to(device=device)\n        inputs2 = inputs2.to(device=device)\n        targets = targets.to(device=device)\n\n        output = model(inputs1, inputs2)\n        predictions = (output > 0.5).float().tolist()  # Convert predictions to list format\n        all_preds.extend(predictions)\n        all_labels.extend(targets.tolist())\n        \n    return all_preds, all_labels\n\n\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\n\ndef build_confusion_matrix(preds, labels):\n    # Build confusion matrix\n    cm = confusion_matrix(preds, labels)\n    \n    # Normalize the confusion matrix\n    cm_percentage = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n\n    # Define the labels\n    labels = ['Genuine', 'Forged']\n    # Plot the confusion matrix\n    sns.heatmap(cm_percentage, annot=True, fmt='.1f', cmap='Blues')\n    plt.gca().set_xticklabels(labels, rotation=0)\n    plt.gca().set_yticklabels(labels, rotation=0)\n    # Set the axis labels and title\n    plt.xlabel('Predicted Labels')\n    plt.ylabel('True Labels')\n    plt.title('Confusion Matrix (Percentage)')\n\n    # Display the plot\n    plt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-07-22T11:22:37.068712Z","iopub.status.idle":"2023-07-22T11:22:37.069767Z","shell.execute_reply.started":"2023-07-22T11:22:37.069507Z","shell.execute_reply":"2023-07-22T11:22:37.069529Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report","metadata":{"execution":{"iopub.status.busy":"2023-07-22T11:22:37.071043Z","iopub.status.idle":"2023-07-22T11:22:37.072265Z","shell.execute_reply.started":"2023-07-22T11:22:37.071965Z","shell.execute_reply":"2023-07-22T11:22:37.071988Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds, labels = get_predictions(validation_loader, model_rms)\nprint(classification_report(preds, labels))\nbuild_confusion_matrix(preds, labels)","metadata":{"execution":{"iopub.status.busy":"2023-07-22T11:22:37.07357Z","iopub.status.idle":"2023-07-22T11:22:37.074743Z","shell.execute_reply.started":"2023-07-22T11:22:37.074484Z","shell.execute_reply":"2023-07-22T11:22:37.074507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cedar_dataset = '/kaggle/input/handwritten-signature-datasets/CEDAR/CEDAR'\ncedar_duplet = duplet_dataset_preparation(cedar_dataset)\n\n# cedar_duplet = cedar_duplet.sample(10000, random_state=42)\n\ntransformation = transforms.Compose([\n    transforms.Resize((200,300)),\n#     transforms.Grayscale(num_output_channels=3),\n    transforms.ToTensor(),\n])\ncedar_dataset = DupletDataset(cedar_duplet, transform=transformation)\n\nvalidation_loader = torch.utils.data.DataLoader(\n    cedar_dataset, batch_size=32\n)\npreds, labels = get_predictions(validation_loader, model_rms)\nprint(classification_report(preds, labels))\nbuild_confusion_matrix(preds, labels)","metadata":{"execution":{"iopub.status.busy":"2023-07-22T11:22:37.076005Z","iopub.status.idle":"2023-07-22T11:22:37.077177Z","shell.execute_reply.started":"2023-07-22T11:22:37.076923Z","shell.execute_reply":"2023-07-22T11:22:37.076946Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cedar_dataset = '/kaggle/input/signature-forgery-dataset/Test'\ncedar_duplet = duplet_dataset_preparation(cedar_dataset)\n\n\n# cedar_duplet = cedar_duplet.sample(10000, random_state=42)\n\ntransformation = transforms.Compose([\n    transforms.Resize((200,300)),\n    transforms.ToTensor(),\n])\ncedar_dataset = DupletDataset(cedar_duplet, transform=transformation)\n\nvalidation_loader = torch.utils.data.DataLoader(\n    cedar_dataset, batch_size=32\n)\npreds, labels = get_predictions(validation_loader, model_rms)\nprint(classification_report(preds, labels))\nbuild_confusion_matrix(preds, labels)","metadata":{"execution":{"iopub.status.busy":"2023-07-22T11:22:37.078445Z","iopub.status.idle":"2023-07-22T11:22:37.080353Z","shell.execute_reply.started":"2023-07-22T11:22:37.080101Z","shell.execute_reply":"2023-07-22T11:22:37.080127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cedar_dataset = '/kaggle/input/handwritten-signature-datasets/BHSig260-Hindi/BHSig260-Hindi'\ncedar_duplet = duplet_dataset_preparation(cedar_dataset)\n\ncedar_duplet = cedar_duplet.sample(12000, random_state=42)\n\ntransformation = transforms.Compose([\n    transforms.Resize((200,300)),\n#     transforms.Grayscale(num_output_channels=3),\n    transforms.ToTensor(),\n])\ncedar_dataset = DupletDataset(cedar_duplet, transform=transformation)\n\nvalidation_loader = torch.utils.data.DataLoader(\n    cedar_dataset, batch_size=32\n)\npreds, labels = get_predictions(validation_loader, model_rms)\nprint(classification_report(preds, labels))\nbuild_confusion_matrix(preds, labels)","metadata":{"execution":{"iopub.status.busy":"2023-07-22T11:22:37.082005Z","iopub.status.idle":"2023-07-22T11:22:37.082452Z","shell.execute_reply.started":"2023-07-22T11:22:37.082221Z","shell.execute_reply":"2023-07-22T11:22:37.082242Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cedar_dataset = '/kaggle/input/handwritten-signature-datasets/BHSig260-Bengali/BHSig260-Bengali'\ncedar_duplet = duplet_dataset_preparation(cedar_dataset)\n\ncedar_duplet = cedar_duplet.sample(12000, random_state=42)\n\ntransformation = transforms.Compose([\n    transforms.Resize((200,300)),\n#     transforms.Grayscale(num_output_channels=3),\n    transforms.ToTensor(),\n])\ncedar_dataset = DupletDataset(cedar_duplet, transform=transformation)\n\nvalidation_loader = torch.utils.data.DataLoader(\n    cedar_dataset, batch_size=32\n)\npreds, labels = get_predictions(validation_loader, model_rms)\nprint(classification_report(preds, labels))\nbuild_confusion_matrix(preds, labels)","metadata":{"execution":{"iopub.status.busy":"2023-07-22T11:22:37.084494Z","iopub.status.idle":"2023-07-22T11:22:37.085628Z","shell.execute_reply.started":"2023-07-22T11:22:37.085387Z","shell.execute_reply":"2023-07-22T11:22:37.085411Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_predictions_for_single(img1, img2, model):\n    # Load the images\n    image1 = Image.open(img1)\n    plt.imshow(image1)\n    plt.show()\n    image1 = preprocess_image(image1)\n    image2 = Image.open(img2)\n    plt.imshow(image2)\n    plt.show()\n    image2 = preprocess_image(image2)\n    \n    \n    \n    # Preprocess the images\n    transform = transforms.Compose([\n        transforms.Resize((200,300)),\n#         transforms.Grayscale(num_output_channels=3),\n        transforms.ToTensor(),\n    ])\n    input1 = transform(image1).unsqueeze(0).to(device)\n    input2 = transform(image2).unsqueeze(0).to(device)\n    \n    # Make predictions using the model\n    model.eval()\n    with torch.no_grad():\n        predictions = model(input1, input2)\n        print(predictions)\n\n    # Determine the prediction label\n    if predictions < 0.2:\n        print(\"The signatures are similar.\")\n    else:\n        print(\"The signatures are not similar.\")\n","metadata":{"execution":{"iopub.status.busy":"2023-07-22T11:22:37.086914Z","iopub.status.idle":"2023-07-22T11:22:37.087501Z","shell.execute_reply.started":"2023-07-22T11:22:37.087271Z","shell.execute_reply":"2023-07-22T11:22:37.087293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img1 = \"/kaggle/input/signature-forgery-dataset/Test/2/original_02_03.jpg\"\nimg2 = \"/kaggle/input/signature-forgery-dataset/Test/3/original_03_03.jpg\"\nget_predictions_for_single(img1, img2, model_rms)","metadata":{"execution":{"iopub.status.busy":"2023-07-22T11:22:37.089331Z","iopub.status.idle":"2023-07-22T11:22:37.089871Z","shell.execute_reply.started":"2023-07-22T11:22:37.089613Z","shell.execute_reply":"2023-07-22T11:22:37.089636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nfrom scipy.ndimage import rotate\ndef test_preprocess_image(image_pth,delta=1, limit=10):\n    \n    def determine_score(arr, angle):\n        data = rotate(arr, angle, reshape=False, order=0)\n        histogram = np.sum(data, axis=1, dtype=float)\n        score = np.sum((histogram[1:] - histogram[:-1]) ** 2, dtype=float)\n        return histogram, score\n    \n    # Convert image to grayscale\n    gray = image_pth.convert(\"L\")\n\n    # Convert grayscale image to numpy array\n    img = np.array(gray)\n    # Apply median blur\n    blur = cv2.medianBlur(img,3)\n\n    # Define kernel for morphology operations\n    kernel = cv2.getStructuringElement(cv2.MORPH_CROSS, (3, 3))\n\n    # Perform erosion\n    erode = cv2.erode(blur, kernel, iterations=2)\n\n    # Perform dilation\n    dilate = cv2.dilate(erode, kernel, iterations=1)\n\n    # Apply thresholding\n    _, binary = cv2.threshold(dilate, 128, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n    # Find the bounding box coordinates of the non-white pixels\n    coords = cv2.findNonZero(binary)\n    x, y, w, h = cv2.boundingRect(coords)\n\n    # Add extra white space to the bounding box coordinates\n    padding = 20  # Adjust the padding size as needed\n    x -= padding\n    y -= padding\n    w += 2 * padding\n    h += 2 * padding\n\n    # Make sure the coordinates are within the image boundaries\n    x = max(0, x)\n    y = max(0, y)\n    w = min(w, img.shape[1] - x)\n    h = min(h, img.shape[0] - y)\n\n    # Crop the image using the modified bounding box coordinates\n    cropped_image = binary[y:y+h, x:x+w]\n\n    # Add extra white space around the cropped image\n    extra_space = np.zeros((cropped_image.shape[0] + 2 * padding, cropped_image.shape[1] + 2 * padding), dtype=np.uint8) * 255\n    extra_space[padding:-padding, padding:-padding] = cropped_image\n    \n    scores = []\n    angles = np.arange(-limit, limit + delta, delta)\n    for angle in angles:\n        histogram, score = determine_score(extra_space, angle)\n        scores.append(score)\n\n    best_angle = angles[scores.index(max(scores))]\n\n    (h, w) = extra_space.shape[:2]\n    center = (w // 2, h // 2)\n    M = cv2.getRotationMatrix2D(center, best_angle, 1.0)\n    corrected = cv2.warpAffine(extra_space, M, (w, h), flags=cv2.INTER_CUBIC, borderMode=cv2.BORDER_REPLICATE)\n    # Convert the numpy array back to PIL image\n    resized_image = Image.fromarray(corrected)\n\n    return resized_image","metadata":{"execution":{"iopub.status.busy":"2023-07-22T11:22:37.091846Z","iopub.status.idle":"2023-07-22T11:22:37.092316Z","shell.execute_reply.started":"2023-07-22T11:22:37.09208Z","shell.execute_reply":"2023-07-22T11:22:37.092102Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Calculating Predictions Using Voting \n\n### 1. First Creating a test dataset","metadata":{}},{"cell_type":"code","source":"import random\n\ndef test_dataset(dataset_folder):\n    \"\"\"\n    This function returns a dataframe\n    Dataframe consists of ClassName, Testing Image and Label\n    \"\"\"\n    df = pd.DataFrame(columns=['ClassName', 'Image', 'Label'])\n\n    for person_folder in os.listdir(dataset_folder):\n        class_name = str(person_folder)\n        person_folder_path = os.path.join(dataset_folder, person_folder)\n        images = os.listdir(person_folder_path)\n        selected_images = random.sample(images, k=min(5, len(images)))  # Randomly select up to five images\n\n        for image in selected_images:\n            image_path = os.path.join(person_folder_path, image)\n            is_genuine = True if 'original' in image or '-G-' in image else False\n            label = 0 if is_genuine else 1\n            df = df.append({'ClassName': class_name, 'Image': image_path, 'Label': label}, ignore_index=True)\n\n    return df","metadata":{"execution":{"iopub.status.busy":"2023-07-22T11:22:37.094091Z","iopub.status.idle":"2023-07-22T11:22:37.094574Z","shell.execute_reply.started":"2023-07-22T11:22:37.094319Z","shell.execute_reply":"2023-07-22T11:22:37.094342Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_genuine_with_class(dataset_folder):\n    \"\"\"\n    This function returns a dataframe consisting of 4 genuine images \n    for each class\n    \"\"\"\n    # Create an empty DataFrame\n    df = pd.DataFrame(columns=['GenuineImages', 'Class'])\n\n    for class_folder in os.listdir(dataset_folder):\n        class_folder_path = os.path.join(dataset_folder, class_folder)\n\n        if not os.path.isdir(class_folder_path):\n            continue  # Skip non-directory files\n\n        genuine_images = []\n        counter = 0  # Counter for tracking the number of genuine images added\n\n        for filename in os.listdir(class_folder_path):\n            if 'original' in filename or '-G-' in filename:\n                genuine_images.append(filename)\n                counter += 1  # Increment the counter\n\n                if counter == 5:\n                    break  # Break out of the loop if 4 genuine images have been added\n        # Add genuine image paths to DataFrame\n        data = []\n        for image in genuine_images:\n            image_path = os.path.join(class_folder_path, image)\n            data.append([image_path, class_folder])\n\n        df = df.append(pd.DataFrame(data, columns=['GenuineImages', 'Class']), ignore_index=True)\n\n    return df","metadata":{"execution":{"iopub.status.busy":"2023-07-22T11:22:37.09629Z","iopub.status.idle":"2023-07-22T11:22:37.096767Z","shell.execute_reply.started":"2023-07-22T11:22:37.096509Z","shell.execute_reply":"2023-07-22T11:22:37.096541Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_genuine_and_test_images_for_class(df, class_id, test_image):\n    class_images = df.loc[df['Class'] == class_id, 'GenuineImages']\n    test_images = [test_image] * len(class_images)\n    class_images = pd.DataFrame({'GenuineImages': class_images, 'TestImage': test_images})\n    return class_images","metadata":{"execution":{"iopub.status.busy":"2023-07-22T11:22:37.098468Z","iopub.status.idle":"2023-07-22T11:22:37.098943Z","shell.execute_reply.started":"2023-07-22T11:22:37.098712Z","shell.execute_reply":"2023-07-22T11:22:37.098735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class GenuineTestDataset(Dataset):\n    def __init__(self, df, transform=None):\n        self.df = df\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, index):\n        row = self.df.iloc[index]\n        genuine_image_path = row['GenuineImages']\n        test_image_path = row['TestImage']\n\n        genuine_image = Image.open(genuine_image_path)\n        test_image = Image.open(test_image_path)\n        \n        genuine_image = test_preprocess_image(genuine_image)\n        test_image = test_preprocess_image(test_image)\n        \n        if self.transform is not None:\n            genuine_image = self.transform(genuine_image)\n            test_image = self.transform(test_image)\n\n        return genuine_image, test_image\n","metadata":{"execution":{"iopub.status.busy":"2023-07-22T11:22:37.100882Z","iopub.status.idle":"2023-07-22T11:22:37.101392Z","shell.execute_reply.started":"2023-07-22T11:22:37.101148Z","shell.execute_reply":"2023-07-22T11:22:37.101171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def perform_voting(model, dataloader):\n    model.eval()  # Set the model to evaluation mode\n    positive_votes = 0\n    negative_votes = 0\n    \n    # Iterate over the dataset\n    for i, data in tqdm(enumerate(dataloader, 0)):\n        genuine_image, test_image = data\n        genuine_image = genuine_image.to(device)\n        test_image = test_image.to(device)\n        \n        with torch.no_grad():\n            output = model(genuine_image, test_image)\n            if output < 0.4:\n                positive_votes += 1\n            else:\n                negative_votes += 1\n    # Determine the final prediction based on voting\n    if positive_votes > negative_votes:\n        final_prediction = 0   # Genuine\n    else:\n        final_prediction = 1   # Forged\n    \n    return final_prediction\n","metadata":{"execution":{"iopub.status.busy":"2023-07-22T11:22:37.103497Z","iopub.status.idle":"2023-07-22T11:22:37.104599Z","shell.execute_reply.started":"2023-07-22T11:22:37.104351Z","shell.execute_reply":"2023-07-22T11:22:37.104374Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def peform_voting_for_each_test_images(folder):\n#     folder = '/kaggle/input/handwritten-signature-datasets/CEDAR/CEDAR'\n    test_df = get_genuine_with_class(folder)\n    \n    test_images = test_dataset(folder)\n    all_preds = []\n    all_labels = []\n    \n    for index, row in test_images.iterrows():\n        class_name = row['ClassName']\n        image_path = row['Image']\n        label = row['Label']\n        desired_df = get_genuine_and_test_images_for_class(test_df,class_name, image_path)\n    \n        # Create an instance of the GenuineTestDataset\n        dataset = GenuineTestDataset(desired_df, transform=transformation)\n\n        # Create a DataLoader for batching and loading the data\n        dataloader = DataLoader(dataset, batch_size=1)\n        predictions = perform_voting(model_rms, dataloader)\n        all_preds.append(predictions)\n        all_labels.append(label)\n        print(f\"{class_name}\\t{image_path}\\t{label}\\t{predictions}\")\n        \n    return all_preds, all_labels","metadata":{"execution":{"iopub.status.busy":"2023-07-22T11:22:37.105886Z","iopub.status.idle":"2023-07-22T11:22:37.107034Z","shell.execute_reply.started":"2023-07-22T11:22:37.106783Z","shell.execute_reply":"2023-07-22T11:22:37.106806Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"folder = '/kaggle/input/handwritten-signature-datasets/CEDAR/CEDAR'\npreds,labels = peform_voting_for_each_test_images(folder)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-07-22T11:22:37.108318Z","iopub.status.idle":"2023-07-22T11:22:37.110512Z","shell.execute_reply.started":"2023-07-22T11:22:37.110215Z","shell.execute_reply":"2023-07-22T11:22:37.110239Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report\nbuild_confusion_matrix(preds,labels)\nprint(classification_report(preds,labels))","metadata":{"execution":{"iopub.status.busy":"2023-07-22T11:22:37.112063Z","iopub.status.idle":"2023-07-22T11:22:37.112849Z","shell.execute_reply.started":"2023-07-22T11:22:37.112581Z","shell.execute_reply":"2023-07-22T11:22:37.112603Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"confusion_matrix(preds,labels)","metadata":{"execution":{"iopub.status.busy":"2023-07-22T11:22:37.114221Z","iopub.status.idle":"2023-07-22T11:22:37.114999Z","shell.execute_reply.started":"2023-07-22T11:22:37.114764Z","shell.execute_reply":"2023-07-22T11:22:37.114787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"folder = '/kaggle/input/handwritten-signature-datasets/BHSig260-Hindi/BHSig260-Hindi'\npreds,labels = peform_voting_for_each_test_images(folder)\n\n","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-07-22T11:22:37.116378Z","iopub.status.idle":"2023-07-22T11:22:37.117159Z","shell.execute_reply.started":"2023-07-22T11:22:37.116908Z","shell.execute_reply":"2023-07-22T11:22:37.116931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"build_confusion_matrix(preds,labels)\nprint(classification_report(preds,labels))","metadata":{"execution":{"iopub.status.busy":"2023-07-22T11:22:37.118545Z","iopub.status.idle":"2023-07-22T11:22:37.119337Z","shell.execute_reply.started":"2023-07-22T11:22:37.119097Z","shell.execute_reply":"2023-07-22T11:22:37.119119Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"confusion_matrix(preds,labels)","metadata":{"execution":{"iopub.status.busy":"2023-07-22T11:22:37.120686Z","iopub.status.idle":"2023-07-22T11:22:37.121509Z","shell.execute_reply.started":"2023-07-22T11:22:37.121269Z","shell.execute_reply":"2023-07-22T11:22:37.121292Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"folder = '/kaggle/input/handwritten-signature-datasets/BHSig260-Bengali/BHSig260-Bengali'\npreds,labels = peform_voting_for_each_test_images(folder)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-07-22T11:22:37.122872Z","iopub.status.idle":"2023-07-22T11:22:37.123627Z","shell.execute_reply.started":"2023-07-22T11:22:37.123387Z","shell.execute_reply":"2023-07-22T11:22:37.12341Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"build_confusion_matrix(preds,labels)\nprint(classification_report(preds,labels))\nconfusion_matrix(preds,labels)","metadata":{"execution":{"iopub.status.busy":"2023-07-22T11:22:37.12523Z","iopub.status.idle":"2023-07-22T11:22:37.126143Z","shell.execute_reply.started":"2023-07-22T11:22:37.125854Z","shell.execute_reply":"2023-07-22T11:22:37.12588Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"folder = '/kaggle/input/signature-forgery-dataset/Test'\npreds,labels = peform_voting_for_each_test_images(folder)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-07-22T11:22:37.127728Z","iopub.status.idle":"2023-07-22T11:22:37.128551Z","shell.execute_reply.started":"2023-07-22T11:22:37.128303Z","shell.execute_reply":"2023-07-22T11:22:37.128328Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"build_confusion_matrix(preds,labels)\nprint(classification_report(preds,labels))\nconfusion_matrix(preds,labels)","metadata":{"execution":{"iopub.status.busy":"2023-07-22T11:22:37.130129Z","iopub.status.idle":"2023-07-22T11:22:37.130937Z","shell.execute_reply.started":"2023-07-22T11:22:37.13067Z","shell.execute_reply":"2023-07-22T11:22:37.130694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## On Train\nfolder = '/kaggle/input/signature-forgery-dataset/Train'\npreds,labels = peform_voting_for_each_test_images(folder)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-07-22T11:22:37.132404Z","iopub.status.idle":"2023-07-22T11:22:37.133299Z","shell.execute_reply.started":"2023-07-22T11:22:37.133025Z","shell.execute_reply":"2023-07-22T11:22:37.133073Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"build_confusion_matrix(preds,labels)\nprint(classification_report(preds, labels))","metadata":{"execution":{"iopub.status.busy":"2023-07-22T11:22:37.134918Z","iopub.status.idle":"2023-07-22T11:22:37.13576Z","shell.execute_reply.started":"2023-07-22T11:22:37.135487Z","shell.execute_reply":"2023-07-22T11:22:37.135511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Single ","metadata":{}},{"cell_type":"code","source":"def perform_voting(model, dataloader):\n    model.eval()  # Set the model to evaluation mode\n    positive_votes = 0\n    negative_votes = 0\n    \n    # Iterate over the dataset\n    for i, data in tqdm(enumerate(dataloader, 0)):\n        genuine_image, test_image = data        \n        genuine_image = genuine_image.to(device)\n        test_image = test_image.to(device)\n        \n        with torch.no_grad():\n            output = model(genuine_image, test_image)\n            print(output)\n            if output < 0.4:\n                positive_votes += 1\n            else:\n                negative_votes += 1\n    \n    print(f\"Positive Votes: {positive_votes} \\t Negative votes: {negative_votes}\")\n    # Determine the final prediction based on voting\n    if positive_votes > negative_votes:\n        final_prediction = \"Genuine\"\n    else:\n        final_prediction = \"Forged\"\n\n    return final_prediction\n","metadata":{"execution":{"iopub.status.busy":"2023-07-22T11:22:37.137345Z","iopub.status.idle":"2023-07-22T11:22:37.138139Z","shell.execute_reply.started":"2023-07-22T11:22:37.137887Z","shell.execute_reply":"2023-07-22T11:22:37.13791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"folder = '/kaggle/input/signature-forgery-dataset/Test'\ntest_df = get_genuine_with_class(folder)\ntest_image = '/kaggle/input/signature-forgery-dataset/Test/3/forge_03_02.jpg'\ndesired_df = get_genuine_and_test_images_for_class(test_df, \"23\",test_image)\n# Create an instance of the GenuineTestDataset\ndataset = GenuineTestDataset(desired_df, transform=transformation)\n\n# Create a DataLoader for batching and loading the data\ndataloader = DataLoader(dataset, batch_size=1)\n\n\nlen(dataloader)\nperform_voting(model_rms, dataloader)","metadata":{"execution":{"iopub.status.busy":"2023-07-22T11:22:37.139657Z","iopub.status.idle":"2023-07-22T11:22:37.140541Z","shell.execute_reply.started":"2023-07-22T11:22:37.140273Z","shell.execute_reply":"2023-07-22T11:22:37.140298Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}